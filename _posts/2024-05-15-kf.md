---
title: "A Python Programmer's Guide to the Kalman Filter"
date: 2015-05-15
---

# A Python Programmer's Guide to the Kalman Filter

Landing humans on the moon is hard. So hard, in fact, that when it was done for the first time back in the late 1960s, mathematicians and engineers needed to use entirely new mathematical constructs to get it done. This post is about the most famous of those constructs: the Kalman Filter.

The Kalman Filter is a method for estimating some physical value or values from (1) imperfect measurements and (2) a model of the underlying system. In the case of the first moon landing, a Kalman Filter was used to [estimate the position and velocity of the lunar module](https://ntrs.nasa.gov/api/citations/19670025568/downloads/19670025568.pdf) in transit between the Earth and the Moon using (1) radar and camera data as well as (2) a mathematical model of orbital dynamics applicable near the Earth/Moon. It was probably used for other things too because, as it turns out, the Kalman Filter is *incredibly useful*.

Introductions to the Kalman Filter abound; I will attempt to distinguish this one in two ways. First, by using Python instead of math notation. This has the advantage of allowing readers to run and modify the examples themselves, which, by the way, I highly recommend that you do. (Also, selfishly, I find it unbearably dull to write out complex expressions in LaTeX.) Second, I will do my best to impart an intuitive understanding of the Kalman Filter. Math and code aside, if you don't understand how this incredible tool works, it will be difficult to put it into practice.

## A Simple Example

Let's lay out a scenario that's in need of a Kalman Filter. We'll stick with the lunar lander theme, but simplify things to make the theory more clear. Imagine that your team is responsible for getting some lunar astronauts from the surface of the moon back into lunar orbit using the lunar module's ascent stage. To make it simple, let's imagine that all this ascent stage needs to do is go up. We will ignore all sideways motion. Your task is to design a system that reports the current height and upward velocity of the vehicle with maximum possible accuracy. In our scenario, the ascent stage's rocket motor imparts an upward (positive) acceleration that increases over time.

// Sketch of the scenario.
![Sketch of the scenario.](../_images/2015-05-15-kf/apollo17.jpg)
*A sketch of our scenario. Image borrowed from [one of my clips](https://www.britannica.com/video/23184/video-mission-Apollo-Moon-astronauts-half-parts).*

```python
dt = 0.1  # Time step (100 ms).
n = 100   # Scenario duration (in time steps).
t = np.linspace(0, dt * n, n)  # Define sampling points for our scenario.
true_acceleration = 1 + 0.01 * np.arange(n)  # Linearly increasing acceleration.
true_velocity = np.cumsum(true_acceleration * dt)  # Integrate acceleration to get velocity.
true_height = np.cumsum(true_velocity * dt)  # Integrate velocity to get height.
```

We're going to cover three approaches to solve this problem, each building off of the last, ending in a full-on Kalman Filter.

## Approach #1: Using Sensors

The naive approach is to use sensors that can just tell us our height and velocity. (In reality we would more likely measure acceleration than velocity, but velocity makes for simpler math, so work with me.)

```python
height_measurements = true_height
velocity_measurements = true_velocity
```

![idealized](../_images/2015-05-15-kf/idealized.jpg)
*Idealized height and velocity measurements.*

Done. But wait, this assumes that our sensors are perfect. Unfortunately, such sensors do not exist. All sensors are subject to sensor noise, as well as a host of other imperfections that we will ignore for the time being. Accounting, for noise our output looks more like this.

```python
import numpy as np

height_variance = 5
velocity_variance = 1

height_measurements = true_height + np.random.normal(0, np.sqrt(height_variance), n)
velocity_measurements = true_velocity + np.random.normal(0, np.sqrt(velocity_variance), n)
```

![sensors](../_images/2015-05-15-kf/sensors.jpg)
*Height and velocity measurements with sensor noise.*

```
Height Error:
  Sensor only:    2.034 m (RMS)
Velocity Error:
  Sensor only:    0.949 m/s (RMS)
```

## Approach #2: A Basic Kalman Filter

Now, this may be good enough, depending on what sort of accuracy we require. But we can do better. For one thing, we can recognize and exploit the fact that the two components of our state vector, height and upward velocity, are related to each other. If we know what our height and upward velocity were at time step ```k-1``` (the previous time step, ```dt``` in the past), then we can _predict_ what the height should be at time step ```k``` (right now).

### Predicting the future

A good prediction is that height at time step ```k``` will be equal to height at ```k-1``` plus upward velocity at ```k-1``` multiplied by the interval between ```k-1``` and ```k```, which we have called ```dt```. We can capture this sort of relationship between state variables with a _state transition matrix_ ```A```. We are also going to combine the two values that we care about, height and upward velocity, into a single vector ```x```, which we will call the _state vector_.

```python
A = np.array([  # State transition matrix.
	[1, dt],
	[0, 1]
])

h = 10    # initial height: 10 m
v = 1     # initial velocity: 1 m/s

x = np.array([  # Initial state vector.
	[h],
	[v]
])
x = A @ x  # Propagate state x forward by one time step according to our model A.
```

Let's expand this equation into its scalar form and take a closer look.

```python
# Equivalent to x = A @ x
h = h + dt * v
v = v
```

Sure enough, this is exactly the relationship for height that was described in the previous paragraph. Note that our equation for v assumes that v does not change over time, which, generally, is not a reliable assumption. But remember, this is just a prediction, and for now it's the best prediction that we have, since our height at time step ```k-1``` tells us nothing about our velocity at time step ```k```. (Later on we will look at a way to improve our velocity prediction when we discuss control inputs.)

So we have our prediction for height and velocity, but we're not done. We also want to quantify how confident we are in our prediction. This confidence, represented by ```P``` (the *prediction covariance matrix*) depends on how sure we were of our previous best guess, as well as how much uncertainty there is in our model of the underlying system dynamics ```Q``` (the *process covariance matrix*).

```python
P = np.array([  # Initial state covariance.
	[1, 0],
	[0, 1]
])
P = A @ P @ A.T + Q  # Propagate state covariance P forward by one time step according to our model A.
```

In summary, our prediction is driven by our knowledge of the underlying system dynamics: specifically it makes use of the fact that upward velocity is the time rate of change of height. What has been described here is the **first step** of a Kalman Filter, called the *prediction step*.

```python
# Predict subsequent state and covariance.
def prediction_step(x, P, A, Q):
	x = A @ x
	P = A @ P @ A.T + Q
	return x, P
```

But there's a **second step**: the _correction step_.

### Correcting our prediction

In the prediction step we predicted the state vector at ```k``` using our previous measurements (sensor readings) and a model of the system dynamics. In the correction step we take new measurements and combine them with our prediction to yield our final estimate of ```x``` at ```k```. How do we combine them? Well, the short answer is that we form a weighted sum of prediction and measurements with higher weight being given for lower variance (higher certainty). The long answer is...

We start by calculating the measurement residual ```y```, which is the difference between our actual measurements ```z``` and the predicted measurements.

```python
z = np.array([
	[height_measurements[k]],
	[velocity_measurements[k]]
])
H = np.array([  # Observation matrix.
	[1, 0],
	[0, 1]
])
y = z - H @ x  # Measurement residual.
```

Note the appearance of a new matrix: ```H```. This is called the *observation matrix*, and it is responsible for mapping between state variables and the sensor readings that measure them. Since we are measuring our state variables height and velocity directly, using height and velocity sensors, ```H``` is just the identity matrix. Working with non-ideal sensors is a very real and very complex topic in engineering, but as it isn't much help in understanding the core intuition behind Kalman filtering, I am going to gloss over it here. Wherever you see it, you can just think of it as simply multiplying by 1.

We then compute the residual covariance ```S``` as the sum of the prediction covariance and the measurement covariance ```R```.

```python
R = np.array([  # Measurement covariance.
	[height_variance, 0],
	[0, velocity_variance]
])
S = H @ P @ H.T + R  # Residual covariance.
```

Next we compute the Kalman gain matrix. As you may have guessed from the name, this matrix is very important, and gets to the heart of what a Kalman filter does: optimally combine all available information (system dynamics and measurements) to form the best possible estimate of the current state. The Kalman gain is the term that encapsulates this optimality. It is computed as follows.

```python
K = P @ H.T @ np.linalg.inv(S)  # Kalman gain.
```

The Kalman gain is then used to combine the predicted state with the new measurements to form the *state estimate*...

```python
x = x + K @ y  # State estimate.
```

...as well as the *state covariance estimate*.

```python
P = (np.eye(2) - K @ H) @ P  # State covariance estimate.
```

The part where we calculate our state estimate is very important, so let's look at it a little more closely. I'm going to take a few of these lines of code and rewrite them with two simplifications:

1) Ignore ```H``` as discussed above.
2) Treat matrices as scalars (replace @ with *, and np.linalg.inv() with /).

Note that this simplified version is not rigorously true, nor will it run properly with matrices, but it should look a little more intuitive.

```python
# Simplified version of state estimation step.
S = P + R
K = P / S
x = x + K * y
```

We can then rewrite this as...

```python
# Simplified, condensed version of state estimation step.
x = x + (P / (P + R)) * y
```

This says that to get our state estimate ```x``` (on the left) we take our prediction ```x``` (on the right) and multiply our measurements ```y``` by the predicted state covariance ```P``` divided by the sum of predicted state covariance and measurement covariance ```R```. If our predicted state covariance is lower than our measurement covariance, then we are more confident in our prediction than we are in our measurements, and so we weight the prediction more heavily. On the other hand, if measurement covariance is lower, then we are more confident in our measurements, and so we weight them more heavily.

Here is what we get for our trouble:

![basic](../_images/2015-05-15-kf/basic-kf.png)
*Height and velocity estimates using our basic Kalman Filter.*

```
Height Error:
  Sensor only:          2.034 m (RMS)
  Basic Kalman filter:  0.637 m (RMS)
Velocity Error:
  Sensor only:          0.949 m/s (RMS)
  Basic Kalman filter:  0.484 m/s (RMS)
```

Wow! That's a lot better than just trusting our noisy sensors. And there's still more that we can do to improve.

## Attempt #3: A Complete Kalman Filter

Reminder: Our goal is to use all information available to us to make the best possible estimate of height and velocity in the presence of noise. In the last two sections we have shown how we can make use of the information we have about our system dynamics (prediction) as well as the information we get from our measurements (correction). That's great, but there's one more source of information that we haven't yet tapped.

### Incorporating control

Our vehicle is powered by a rocket motor that is under our control. This motor is an actuator: a device that we can command to influence our state. We are telling it what to do, which means that we have knowledge of the acceleration that it is imparting to the vehicle. We can incorporate this information into our estimate.

We do this by adjusting our prediction step to account for the fact that we are actively affecting the system dynamics. Two new terms are required. The first is the *control input*, usually denoted ```u```, which describes what each actuator is doing. Since we have only one actuator (the rocket motor), this is just a scalar rather than a vector. The second is the *control input matrix* ```B```, which describes how the control input affects the state vector. Our rocket motor imparts an upward acceleration, which in our one-dimensional world is the second time derivative of height and the first time derivative of upward velocity.

```python
u = true_acceleration[k]
B = np.array([
	[0.5 * dt**2],
	[dt]
])
```

Note that we're setting control input ```u``` equal to the *true* acceleration. This should raise some eyebrows and, indeed, it's a bit of a lie. What should go here is the *commanded* acceleration, which is subject to actuator noise/bias/etc., and is thus not quite equal to the true acceleration. This is another one of those complications that I'm going to ignore as a distraction from the core concepts. What you should take away is this: we have a very good idea of what control input we're applying because *we're* the ones applying it, and because it's *our* rocket motor that we've had plenty of time to study and calibrate.

OK, all that's left is to incorporate our control into the prediction step.

```python
def prediction_step(x, P, u, A, B, Q):
    x = A @ x + B * u
    P = A @ P @ A.T + Q
    return x, P
```

Now our prediction doesn't just account for the passive dynamics of the system ```A @ x```, it also considers our active control of the dynamics via ```B * u```. This leads to the following:

![full](../_images/2015-05-15-kf/full-kf.png)
*Height and velocity estimates using our full Kalman Filter.*

```
  Sensor only:          2.034 m (RMS)
  Basic Kalman filter:  0.637 m (RMS)
  Kalman filter:        0.589 m (RMS)
Velocity Error:
  Sensor only:          0.949 m/s (RMS)
  Basic Kalman filter:  0.484 m/s (RMS)
  Kalman filter:        0.331 m/s (RMS)
```

Very nice.

## Conclusion

We have looked at three ways to estimate the height and velocity of a lunar spacecraft. First, we slapped a couple of sensors onto our craft and trusted them to tell us the height and velocity directly. This worked, but due to unavoidable sensor noise, the result was mediocre. Next, we used the magic of the Kalman Filter to combine our sensor measurements with an educated prediction of where we *should be* based on where we were, which improved our estimates significantly. Finally, we incorporated information about how the spacecraft was being controlled, which improved our estimates even more. This final approach was a bona fide Kalman Filter, which combined all of the information available to us in order to form the best possible estimates.

## Recommended Further Reading

Now you know the basics, but there's much more to learn. For one thing, the Kalman Filter discussed above only works if the system dynamics (and measurements models) are linear and, as you may well know, linear systems are pretty much nonexistent in real-life engineering. To handle nonlinear systems we must turn to the Kalman Filter's nonlinear cousins: the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF). There are also more exotic state estimators like the Particle Filter that (as of this article's posting) are not yet widely used, but may become prevalent in the future. State estimation is an active field of research.

If you prefer math notation to Python, then this write-up may be more to your taste.
https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/

For a more thorough treatment that puts the Kalman Filter in the context of modern control theory, Professor Steve Brunton has an excellent sequence of (39) lectures on YouTube. While you could just watch the one called "The Kalman Filter", it probably won't make much sense unless you watch the preceding ones.
https://www.youtube.com/playlist?list=PLMrJAkhIeNNR20Mz-VpzgfQs5zrYi085m

For the ambitious among you, this GitHub repository offers a practical and comprehensive view not just of the (linear) Kalman Filter, but of derived and/or related state estimators as well. If you want to become an expert, go here.
https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/tree/master
