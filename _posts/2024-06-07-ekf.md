---
title: "A Python Programmer's Guide to the Extended Kalman Filter"
date: 2024-06-07
excerpt: "An introduction to the Extended Kalman Filter with examples in Python."
---

## A Python Programmer's Guide to the Extended Kalman Filter

In [last week's post]("{{ site.baseurl }}/2024/05/31/kf.html") we presented the linear Kalman Filter and showed how it can be used to estimate state from noisy measurements and an understanding of the underlying system dynamics (this understanding is called a state transition or process model). The results were good, but they relied on a couple of assumptions. First, they assumed that the system had linear dynamics: acceleration is the derivative of velocity is the derivative of height. Second, they assumed that our sensors measured the state variables directly (linearly). Today we're throwing out both of those assumptions.

When dealing with nonlinear measurements or dynamics, we turn to the Extended Kalman Filter (EKF). The idea of the EKF is rather simple: at each time step ```k``` we linearize our measurement and process models using a first order Taylor series expansion, then we apply a linear Kalman Filter to the linearized models. Let's try it.

### A More Complicated Example

We're going to continue with our lunar spacecraft example from last time, but to make things nonlinear we'll give the moon an atmosphere. That's right, we're terraforming the moon. Accounting for atmospheric drag means subtracting a drag term from our calculated velocity. The drag term varies with the square of velocity and exponentially with height, which means it is nonlinear, making it impossible to model with a linear Kalman Filter. We need the EKF.

```python
import numpy as np

dt = 0.1  # Time step
n = 100   # Duration (in time steps)
t = np.linspace(0, dt * n, n)

# Constants/functions for (fictional) drag model
# Adapted from here: https://www.mide.com/air-pressure-at-altitude-calculator
surface_atm_density = 3e-2  # Atmospheric density at the surface (Pa)
calc_atm_density = lambda height: surface_atm_density * (1 - 3e-3 * height) ** 5
def calc_drag(height, velocity):
    atm_density = calc_atm_density(height)
    return 0.5 * atm_density * velocity ** 2

commanded_acceleration = 5 + 0.1 * np.arange(n)
true_acceleration = commanded_acceleration.copy()
true_velocity = np.zeros(n)
true_height = np.zeros(n)

# Account for drag
for k in range(1, n):
    drag = calc_drag(true_height[k-1], true_velocity[k-1])
    true_velocity[k] = true_velocity[k-1] + true_acceleration[k-1] * dt - drag
    true_height[k] = true_height[k-1] + true_velocity[k-1] * dt
```

### Approach #1: Using Sensors

So we've thrown out our assumption of a linear process model. The next step is to throw out our assumption of a linear measurement model. We're going to do that by measuring the square root of height rather than the height itself. I'll admit - I don't have a very good physical backing for this change. As far as I know there are no height sensors that work this way. But, it's an easy nonlinearity to introduce and it fits into the scenario. The important thing to note is that sometimes we can't measure our state variables directly, so the best we can do is to measure things that are related to our state variables, possibly in nonlinear ways.

```python
height_variance = np.sqrt(5)
velocity_variance = 1

height_measurements = np.sqrt(true_height) + np.random.normal(0, np.sqrt(height_variance), n)
velocity_measurements = true_velocity + np.random.normal(0, np.sqrt(velocity_variance), n)
```

<img src="{{ site.baseurl }}/assets/ekf/sensors.png" width="100%" />
*Height and velocity measurements with sensor noise and a nonlinear sensor model.*

```
Height Error:
  Sensor only:    6.842 m (RMS)
Velocity Error:
  Sensor only:    0.949 m/s (RMS)
```

Our measurements are very noisy, so it's time to break out the Kalman Filter. But wait, a matrix can only describe a linear system. Our nonlinear process model can't be described by a single ```A``` matrix, nor can our nonlinear measurement model be described by a single ```H``` matrix. So, what do we do?

### Approach #2: A Kalman Filter Linearized at Initialization

Well, the simplest thing that we can do is to calculate our ```A``` and ```H``` matrices by linearizing the corresponding models during initialization. For each model, we do this by computing the Jacobian matrix, which for a state vector of length 2 is a 2x2 matrix containing the 4 partial derivatives of the current state variables with respect to the previous state variables. Let's start with the process model.

```python
def process_model(x, u):
    height, velocity = x.flatten()
    drag = calc_drag(height, velocity)
    new_height = height + velocity * dt
    new_velocity = velocity - drag + u * dt
    return np.array([[new_height],
                     [new_velocity]])

def process_model_jacobian(x, u):
    height, velocity = x.flatten()
    
    # Calculate elements of the Jacobian matrix F
    F11 = 1.                                   # Partial derivative of height[k] with respect to height[k-1]
    F12 = dt                                   # Partial derivative of height[k] with respect to velocity[k-1]
    F21 = 2.25e-4 * (1 - 0.003 * height) ** 4  # Partial derivative of velocity[k] with respect to height[k-1]
    F22 = 1 - 0.3 * (1 - 0.003 * height) ** 5  # Partial derivative of velocity[k] with respect to velocity[k-1]
    
    # Construct the Jacobian matrix F
    F = np.array([[F11, F12],
                  [F21, F22]])
    return F
```

OK, so we define our process model using the ```calc_drag``` function defined previously, and then we find the following partial derivatives: current height w.r.t. previous height, current height w.r.t. previous velocity, current velocity w.r.t. previous height, and current velocity w.r.t. previous velocity. Then:

```python
x = np.array([[0.0],
              [0.0]])
A = process_model_jacobian(x, commanded_acceleration[0])
```

These partial derivatives evaluated at our initial conditions give us our ```A``` matrix. We can do the same thing for our measurement model.

```python
def measurement_model(x):
    height, velocity = x.flatten()
    height = max(height, np.finfo(float).eps)
    return np.array([[np.sqrt(height)],
                     [velocity]])

def measurement_model_jacobian(x):
    height, velocity = x.flatten()
    height = max(height, np.finfo(float).eps)
    H = np.array([[0.5 / np.sqrt(height), 0],
                  [0,                     1]])
    return H

x = np.array([[0.0],
              [0.0]])
H = measurement_model_jacobian(x)
```

We now have everything we need to implement a linear Kalman Filter.

```python
# Initial state covariance estimate
P = np.array([[1., 0.],
              [0., 1.]])

# Process noise covariance
Q = np.array([[0.1, 0],
              [0, 0.1]])

# Measurement noise covariance
R = np.array([[height_variance, 0],
              [0, velocity_variance]])

# Control input matrix.
B = np.array([[0.5 * dt**2],
              [dt]])

# Construct our estimator and run it in simulation.
kf = KalmanFilter()
kf.simulate(
    ground_truth=np.vstack((true_height, true_velocity)).T,
    measurements=np.vstack((height_measurements, velocity_measurements)).T,
    state_variable_names=["Height (m)", "Velocity (m/s)"],
    control_inputs=commanded_acceleration,
    t=t, A=A, B=B, Q=Q, H=H, R=R, x_init=x, P_init=P
)
```

<img src="{{ site.baseurl }}/assets/ekf/simulation-1.png" width="100%" />
*A linear Kalman Filter, with process and measurement models linearized at x[0], u[0].*

```
RMS Error Height (m):
  Sensor:    42.079
  Estimator: 48.009
RMS Error Velocity (m/s):
  Sensor:    0.887
  Estimator: 5.408
```

Sad. The height estimation is completely broken. The velocity estimation has the right general shape, but it's clear that something is very wrong. Here's the problem: our linearized models are only useful near the point about which they were linearized. Once we move away from our initial conditions these models are wrong, and they lead our estimator astray. We need a way to keep these models current.

### Approach #3: The Extended Kalman Filter

In essence, the EKF is a Kalman Filter in which we re-linearize our models at each time step. Specifically, we linearize our process model about the previous estimate and our measurement model about our current prediction. In case that explanation wasn't totally clear, I have loudly indicated in the code block below where these linearizations happen.

```python
def _prediction_step(x, P, u, Q, process_model, process_model_jacobian):
    F = process_model_jacobian(x, u)  # !!! Linearize process model about x[k-1].
    x = process_model(x, u)  # Advance state using full, nonlinear process model.
    P = F @ P @ F.T + Q  # Advance state covariance using linearized process model F.
    return x, P

def _correction_step(x, P, z, R, measurement_model, measurement_model_jacobian):
    H = measurement_model_jacobian(x)  # !!! Linearize measurement model about predicted x.
    y = z - measurement_model(x)  # Calculate measurement residual using full, nonlinear measurement model.
    S = H @ P @ H.T + R  # Linearized measurement model H is used in remaining calculations.
    K = P @ H.T @ np.linalg.inv(S)
    x = x + K @ y
    P = (np.eye(len(P)) - K @ H) @ P
    return x, P

def estimate(self, measurements, control_inputs, Q, R, x_init, P_init,
             process_model, process_model_jacobian,
             measurement_model, measurement_model_jacobian):
    estimates = np.empty_like(measurements)
    estimates_cov = np.empty_like(measurements)
    estimates[0] = x_init.flatten()
    estimates_cov[0] = [P_init[0, 0], P_init[1, 1]]
    x, P = x_init, P_init
    for k in range(1, n):
        u = 0 if control_inputs is None else control_inputs[k-1]
        z = measurements[k, :, np.newaxis]  # np.newaxis because we need shape (n, 1) not (n,)
        x, P = self._prediction_step(x, P, u, Q, process_model, process_model_jacobian)
        x, P = self._correction_step(x, P, z, R, measurement_model, measurement_model_jacobian)
        estimates[k] = x.flatten()
        estimates_cov[k] = [P[0, 0], P[1, 1]]
    return estimates, estimates_cov
```

This should look pretty familiar if you've already seen the linear Kalman Filter post, but there are a couple of things to point out.

Firstly, our ```A``` matrix is now called ```F```, which seems to be the convention. This is fair: ```F``` is a linear approximation of a nonlinear model, whereas ```A``` is an actual linear model, but just be aware of the notation. This notational does not get as far as the measurement model: we still use ```H```.

Secondly, note that in calculating our state *prediction* ```x``` in the prediction step and our measurement residual ```y``` in the correction step, we use the nonlinear models. This raises the perfectly reasonable question of "why can't we just use the nonlinear models everywhere?". Well, it's because of ```P```. Note that in both the prediction and correction steps, all calculations that depend directly or indirectly on ```P``` use the linearized models. The reason for this is a little deeper than I want to go in this high-level post, but the gist is this: ```P``` necessarily describes the shape of a *Gaussian distribution*, and when we pass a Gaussian through a nonlinear function the result is, in general, non-Gaussian. Since the Kalman Filter relies on this assumption of Gaussianity, we must restrict ourselves to performing linear operations on ```P```.

Enough explanation, let's see how it works.

<img src="{{ site.baseurl }}/assets/ekf/simulation-2.png" width="100%" />
*An EKF. Process and measurement models are linearized at each time step*

```
RMS Error Height (m):
  Sensor:    45.887
  Estimator: 0.725
RMS Error Velocity (m/s):
  Sensor:    0.887
  Estimator: 0.248
```

Nice. Not much to say here - it works really well. Remember that the *measured* height is actually measuring the square root of the height, while the *true* height is reporting the actual height.

#### "But I don't like doing calculus"

It's true, building the Jacobian involves taking some partial derivatives, which can be a lot of work, or even impossible. For intractable models its possible to calculate the Jacobian at each step numerically instead of analytically. Numerical differentiation is attractive because it's simple and easy to maintain (no need to find new Jacobian if the model changes), but it is susceptible to accuracy and stability issues from the inherent approximation. We're not going to implement this here, but know that it's an option.

### Handling Disturbances

So our EKF works quite well. But remember, measurement noise isn't the only noise source that we need to deal with, there's also process noise, commonly called disturbance. Let's see how our EKF responds to a simulated 2 second rocket engine failure.

```python
true_acceleration[20:40] = 0
```

<img src="{{ site.baseurl }}/assets/ekf/simulation-4.png" width="100%" />
*Introducing a large disturbance.*

```
RMS Error Height (m):
  Sensor:    35.323
  Estimator: 1.110
RMS Error Velocity (m/s):
  Sensor:    0.926
  Estimator: 0.996
```

<img src="{{ site.baseurl }}/assets/ekf/not-great.jpg" width="100%" />
*Dyatlov weighs in.*

Clearly our estimator has too much faith in the process model. The measurements are telling it that something's changed, but it's not convinced. Let's try increasing the process noise covariance to reflect lower trust in the process model.

```python
# Process noise covariance
Q = np.array([[0.1, 0],
              [0,   1]])
```

<img src="{{ site.baseurl }}/assets/ekf/simulation-5.png" width="100%" />
*An EKF that has been tuned for this level of disturbance.*

```
RMS Error Height (m):
  Sensor:    35.323
  Estimator: 0.958
RMS Error Velocity (m/s):
  Sensor:    0.926
  Estimator: 0.549
```

Much better. Again, this is just an example of the sort of tuning that is necessary to get the behavior your want from your estimator. In practice, "tuning" does not mean minimizing RMS error in a single scenario - you will need to run many simulations to explore estimator performance in different possible scenarios and tune until performance is acceptable in all of the ones that you care about. You'll need to consider how much you care about disturbance robustness and how much you care about precision during nominal operation, because there's a fundamental trade-off there.

### Conclusion

In this post, we delved into the complex (and realistic) realm of nonlinear systems and introduced the Extended Kalman Filter (EKF) as a valuable estimation tool. The EKF operates by linearizing nonlinear process and measurement models at each time step, then applying a standard Kalman Filter to these linearized models. In reality, almost every system we aim to design an estimator for is fundamentally nonlinear. Even in the earliest applications of the Kalman Filter, such as the Apollo program, it was actually EKFs that were used. Therefore, while understanding the linear Kalman Filter is essential, the EKF is a far more useful tool.

### Recommended Further Reading

This is another EKF walkthrough using Python: [github link](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/11-Extended-Kalman-Filters.ipynb)