---
title: "A Python Programmer's Guide to the Unscented Kalman Filter"
date: 2024-06-28
excerpt: "An introduction to the Unscented Kalman Filter with examples in Python."
---

## A Python Programmer's Guide to the Unscented Kalman Filter

In the [last post]("{{ site.baseurl }}/2024/06/07/ekf.html") we looked at the Extended Kalman Filter (EKF) as a technique for doing estimation with nonlinear dynamics and/or sensors. In this post we're going to introduce an alternative technique: the Unscented Kalman Filter (UKF). Why, you might ask, do we need another technique for estimation in the presence of nonlinearities? Didn't the EKF work fine? Well, yes, it did. But there are a lot of nonlinear systems out there, and it never hurts to have more than one tool in your tool belt.

We're going to start with a review of our example scenario and the EKF. Next we'll introduce the UKF, and finally we'll compare the two. Let's jump in.

### Reminder: A Nonlinear System with Drag

In our scenario, the goal is to estimate the height and upward velocity of a lunar ascent vehicle from noisy sensor measurements and a model of the system dynamics. We have given the moon an atmosphere to add a nonlinear drag term to the velocity equation.

```python
import numpy as np

dt = 0.1  # Time step
n = 100   # Duration (in time steps)
t = np.linspace(0, dt * n, n)

# Constants/functions for (fictional) drag model
# Adapted from here: https://www.mide.com/air-pressure-at-altitude-calculator
surface_atm_density = 3e-2  # Atmospheric density at the surface (Pa)
calc_atm_density = lambda height: surface_atm_density * (1 - 3e-3 * height) ** 5
def calc_drag(height, velocity):
    atm_density = calc_atm_density(height)
    return 0.5 * atm_density * velocity ** 2

commanded_acceleration = 5 + 0.1 * np.arange(n)
true_acceleration = commanded_acceleration.copy()
true_velocity = np.zeros(n)
true_height = np.zeros(n)

# Account for drag
for k in range(1, n): 
    drag = calc_drag(true_height[k-1], true_velocity[k-1])
    true_velocity[k] = true_velocity[k-1] + true_acceleration[k-1] * dt - drag
    true_height[k] = true_height[k-1] + true_velocity[k-1] * dt
```

<img src="{{ site.baseurl }}/assets/ukf/idealized.png" width="100%" />
*Height and velocity with nonlinear drag dynamics.*

Here is how our EKF performs.

<img src="{{ site.baseurl }}/assets/ukf/simulation-1.png" width="100%" />
*EKF performance.*

```
RMS Error Height (m):
  Sensor:    45.900
  Estimator: 1.095
RMS Error Velocity (m/s):
  Sensor:    0.943
  Estimator: 0.209

runtime: 7.784 ms
```


### Avoiding Calculus

One drawback of the EKF as shown is that it requires the engineer to find the process and measurement model Jacobians. This may be impossible if the models are analytically intractable, and even if it is possible, it's almost always a pain. Accordingly, this is sometimes given as a reason to prefer the UKF over the EKF, since the UKF, as we will show later on, does not require any solving of potentially-intractable differential equations. However, it's also possible to use an EKF without having to perform these calculations, so this is not, by itself, a valid reason to go with the UKF.

The way to skip the calculus with an EKF is to use numerical integration to approximate the Jacobians at each time step. The idea is to run our model (process or measurement) on the state vector ```x```, then to add a tiny value ```epsilon``` and rerun our model on that, and finally to find the difference between the two outputs divided by epsilon.

```python
    def numerical_jacobian(f, x, u=None, epsilon=1e-5):
        n = x.size
        jacobian = np.zeros((n, n))
        perturb = np.eye(n) * epsilon
        for i in range(n):
            if u is None:
                jacobian[:, i] = (f(x + perturb[:, i].reshape(-1, 1)).flatten()\
                                  - f(x - perturb[:, i].reshape(-1, 1)).flatten()) / (2 * epsilon)
            else:
                jacobian[:, i] = (f(x + perturb[:, i].reshape(-1, 1), u).flatten()\
                                  - f(x - perturb[:, i].reshape(-1, 1), u).flatten()) / (2 * epsilon)
        return jacobian
```

This saves us from having to find the Jacobians ourselves. Let's see how well it works.

<img src="{{ site.baseurl }}/assets/ukf/simulation-2.png" width="100%" />
*EKF performance with numerical integration.*

```
RMS Error Height (m):
  Sensor:    45.900
  Estimator: 0.986
RMS Error Velocity (m/s):
  Sensor:    0.943
  Estimator: 0.226  
  
runtime: 14.711 ms
```

Not much different, though it's worth noting that it's a bit slower. That's the price we pay for convenience.

### The Unscented Kalman Filter

Now onto the UKF. Remember: the key challenge with nonlinear Kalman filtering is that the result a Gaussian distribution passed through a nonlinear function is, generally, not Gaussian. Since Kalman filtering only works with Gaussians, we must approximate these results with Gaussians.

The EKF does this by replacing the nonlinear function with a linear approximation - this means that the result will be Gaussian, since the function is no longer nonlinear. Of course, since we're no longer using the nonlinear function itself, the result will be approximate.

The UKF does this by replacing the input Gaussian with a set of discrete points, called sigma points, then passing those points through the true nonlinear function, and then representing the resulting set of points with a new Gaussian. This final step is called the *unscented transform*, named by its creator after a [stick of deodorant](https://ethw.org/First-Hand:The_Unscented_Transform#What%E2%80%99s_with_the_Name_%E2%80%9CUnscented%E2%80%9D?).

#### Choosing sigma points

Let's look at the code. First, how do we choose sigma points to represent a Gaussian given by ```x``` (mean) and ```P``` (covariance)?

```
def _sigma_points(self, x, P, alpha, kappa):
	dim_x = len(x)
	num_sigmas = dim_x * 2 + 1
    lambda_ = alpha ** 2 * (dim_x + kappa) - dim_x
    U = cholesky((lambda_ + dim_x) * P)
    sigmas = np.zeros((num_sigmas, dim_x))
    sigmas[0] = x
    for k in range(dim_x):
        sigmas[k + 1] = x + U[k]
        sigmas[dim_x + k + 1] = x - U[k]
    return sigmas
```

The answer is that we put one point on the mean, and then for each dimension of ```x``` we place a point up a standard deviation and down a standard deviation. This means that we get ```2*n+1``` points, where ```n``` is the number of elements in ```x```. The code looks a little more complicated than this, but really that's all it's doing. Note that there are other ways to choose sigma points, but this seems to be the default, and is the only way we'll discuss here.

#### The unscented transform

Next, after we've passed these sigma points through our nonlinear function, how do we fit a Gaussian?

```python
def _compute_weights(dim_x, alpha, beta, kappa):
    lambda_ = alpha ** 2 * (dim_x + kappa) - dim_x
    c = 0.5 / (dim_x + lambda_)
    wm = np.full(2 * dim_x + 1, c)
    wc = np.copy(wm)
    wm[0] = lambda_ / (dim_x + lambda_)
    wc[0] = lambda_ / (dim_x + lambda_) + (1 - alpha ** 2 + beta)
    return wm, wc

def _unscented_transform(sigmas, wm, wc, noise_cov):
    x = np.dot(wm, sigmas)
    y = sigmas - x[np.newaxis, :]
    P = np.dot(y.T, np.dot(np.diag(wc), y)) + noise_cov
    return x, P
```

We calculate sets of weights ```wm``` (for the mean) and ```wc``` (for the covariance) and use those weights to apply the unscented transform, which gives us a mean and covariance, which together define the output Gaussian. The ```noise_cov```, or noise covariance, can be either the process noise (in the case of the process model) or the measurement noise (in the case of the measurement model), as we will see in the following subsections. We can see that there really isn't much to the unscented transform, its just addition, subtraction, and a couple of dot products.

#### Prediction

```python
def _prediction_step(x, P, Q, process_model, u, wm, wc):
    sigmas = _sigma_points(x, P)
    sigmas_f = np.zeros((sigmas.shape))
    for i, s in enumerate(sigmas):
        sigmas_f[i] = process_model(s, u)[:, 0]

    x_pred, P_pred = self._unscented_transform(sigmas_f, wm, wc, Q)
    
    return x_pred, P_pred
```

In the prediction step we calculate our sigma points, pass them through the (nonlinear) process model, and use the unscented transform to return a Gaussian prediction.

#### Correction

```python
def _cross_variance(x, z, sigmas_f, sigmas_h, wc):
    Pxz = np.zeros((sigmas_f.shape[1], sigmas_h.shape[1]))
    N = sigmas_f.shape[0]
    for i in range(N):
        dx = sigmas_f[i] - x
        dz = sigmas_h[i] - z
        Pxz += wc[i] * np.outer(dx, dz)
    return Pxz

def _correction_step(x_pred, P_pred, z, R, measurement_model):
    sigmas_f = _sigma_points(x_pred, P_pred)
    sigmas_h = []
    for s in sigmas_f:
        sigmas_h.append(measurement_model(s)[:, 0])

    sigmas_h = np.atleast_2d(sigmas_h)

    zp, S = _unscented_transform(sigmas_h, wm, wc, R)

    SI = np.linalg.inv(S)

    Pxz = _cross_variance(x_pred, zp, sigmas_f, sigmas_h, wc)

    K = np.dot(Pxz, SI)  # Kalman gain
    y = z - zp  			# residual

    x_est = x_pred + np.dot(K, y)
    P_est = P_pred - np.dot(K, np.dot(S, K.T))
    
    return x_est, P_est
```

In the correction step we again choose sigma points, transform them (this time using the nonlinear measurement model), and use the unscented transform to build a new Gaussian. As with any Kalman Filter, we then do some math to optimally combine the information from our prediction and our measurements. As we have covered this part in detail in previous posts, I will move past it here.

#### Ok, how well does it work?

<img src="{{ site.baseurl }}/assets/ukf/simulation-3.png" width="100%" />
*UKF performance.*

```
RMS Error Height (m):
  Sensor:    45.900
  Estimator: 1.019
RMS Error Velocity (m/s):
  Sensor:    0.943
  Estimator: 0.228

runtime: 33.277 ms
```

Answer: about the same as the EKF. 

### EKF vs UKF

Below is a comparison of the Kalman Filter, EKF, and UKF, with the core estimation problem in each case surrounded by a dotted box. Note that for each filter, the above diagram applies twice: once for the prediction step, with the process model, and once for the correction step, with the measurement model.

<img src="{{ site.baseurl }}/assets/ukf/UKF.drawio.png" width="100%" />

The question of whether to use an EKF or a UKF has no clear, general answer. The conventional wisdom is that UKF is better at handling more significant nonlinearities and wider sampling intervals. So, if your dynamics are highly nonlinear or your update rate is slow, you may have better luck with a UKF. On the other hand, EKF is a bit faster, especially if Jacobians are calculated upfront, analytically. In my experience, it's very difficult to predict in advance which method is going to perform better - you should try both.

Here are some papers in which others have done just that:

https://ieeexplore.ieee.org/document/1243440
This paper studies UKF/EKF for human head and hand orientation using quaternions. It finds that they perform equivalently, but the EKF is to be preferred because it's faster and because quaternion dynamics are linear.

https://www.cambridge.org/core/journals/journal-of-navigation/article/abs/comparison-of-unscented-and-extended-kalman-filters-with-application-in-vehicle-navigation/FACDF62B38D5C183313378312E071DC0
This paper studies UKF/EKF for vehicle navigation. It finds that EKF is both more computationally efficient and more stable.

https://arxiv.org/html/2404.03077v1#S5
This paper studies UKF/EKF for localization (position and velocity estimation) using bluetooth packets. It finds that while performance is similar when the number of packets is high, UKF performs better when the number of packets is low (corresponding to a lower update rate).

### Conclusion

In this post we introduced the Unscented Kalman Filter (UKF) as an alternative to the EKF for performing state estimation with nonlinear process and/or measurement models. We showed how it works, how it's different from the EKF, and discussed why you might want to use one or the other. Finally, we found that the choice between EKF and UKF is complicated and must be made on a case by case basis.

### Recommended Further Reading

Another UKF walkthrough using Python. This one delves quite a bit more into the math, in case you want to understand the internals better: [link](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/10-Unscented-Kalman-Filter.ipynb)

Here's a paper by the creators themselves (I think Uhlmann is generally considered *the* creator). This isn't the original paper, but it's a summary of their work that's better for building an understanding: [link](https://www.cs.ubc.ca/~murphyk/Papers/Julier_Uhlmann_mar04.pdf)