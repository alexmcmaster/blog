---
title: "A Python Programmer's Guide to the Kalman Filter"
date: 2024-05-31
excerpt: "An introduction to the linear Kalman Filter with examples in Python."
---

## A Python Programmer's Guide to the Kalman Filter

Landing humans on the moon is hard. So hard, in fact, that when it was done for the first time back in the late 1960s, mathematicians and engineers needed to use entirely new mathematical constructs to get it done. This post is about the most famous of those constructs: the Kalman Filter.

The Kalman Filter is a method for estimating some physical value or values from: (1) imperfect measurements and (2) a mathematical model of the underlying system. In the case of the first moon landing, a Kalman Filter was used to [estimate the position and velocity of the lunar module](https://ntrs.nasa.gov/api/citations/19670025568/downloads/19670025568.pdf) in transit between the Earth and the Moon using: (1) radar and camera data as well as (2) a mathematical model of orbital dynamics applicable near the Earth/Moon. It was probably used for other things too because, as it turns out, the Kalman Filter is *incredibly useful*.

Introductions to the Kalman Filter abound; I will attempt to distinguish this one in two ways. First, by using Python instead of math notation. This has the advantage of allowing readers to run and modify the examples themselves, which, by the way, I highly recommend that you do. (Also, selfishly, I find it unbearably dull to write out complex expressions in LaTeX.) Second, I will do my best to impart an intuitive understanding of the Kalman Filter. Math and code aside, if you don't understand how this incredible tool works, it will be difficult to put it into practice.

Here is the jupyter notebook that goes with this article: [download]({{ site.baseurl }}/code/kf/kalman-filter.ipynb). Feel free to downloaded and tinker with it to your heart's content.

### A Simple Example

Let's lay out a scenario that's in need of a Kalman Filter. We'll stick with the lunar lander theme, but simplify things to make the theory more clear. Imagine that your team is responsible for getting some lunar astronauts from the surface of the moon back into lunar orbit using the lunar module's ascent stage. To make it simple, let's imagine that all this ascent stage needs to do is go up. We will ignore all sideways motion. Your task is to design a system that reports the current height and upward velocity of the vehicle with maximum possible accuracy. In our scenario, the ascent stage's rocket motor imparts an upward (positive) acceleration that increases over time.

<img src="{{ site.baseurl }}/assets/kf/apollo17.jpg" width="100%" />
*A sketch of our scenario. Image borrowed from [one of my favorite clips](https://www.britannica.com/video/23184/video-mission-Apollo-Moon-astronauts-half-parts).*

```python
import numpy as np

dt = 0.1  # Time step
n = 100   # Duration (in time steps)
t = np.linspace(0, dt * n, n)
commanded_acceleration = 2 + 0.01 * np.arange(n)  # Linearly increasing acceleration
true_acceleration = commanded_acceleration.copy()  # Assume (for now) an ideal system

true_velocity = np.cumsum(true_acceleration * dt)  # Integrate acceleration to get velocity
true_height = np.cumsum(true_velocity * dt)  # Integrate velocity to get height
```

We're going to cover three approaches to solve this problem, each building off of the last, ending with a full-on Kalman Filter.

### Approach #1: Using Sensors

The naive approach is to use sensors that can just tell us our height and velocity. (In reality we would more likely measure acceleration than velocity, but velocity makes for simpler math, so work with me.)

```python
height_measurements = true_height
velocity_measurements = true_velocity
```

<img src="{{ site.baseurl }}/assets/kf/idealized.png" width="100%" />
*Idealized height and velocity measurements.*

Done. But wait, this assumes that our sensors are perfect. Unfortunately, such sensors do not exist. All sensors are subject to sensor noise, as well as a host of other imperfections that we will ignore for the time being. Accounting for noise, our output looks more like this.

```python
height_variance = 5
velocity_variance = 1

height_measurements = true_height + np.random.normal(0, np.sqrt(height_variance), n)
velocity_measurements = true_velocity + np.random.normal(0, np.sqrt(velocity_variance), n)
```

<img src="{{ site.baseurl }}/assets/kf/sensors.png" width="100%" />
*Height and velocity measurements with sensor noise.*

That's a lot of noise. To be precise:

```
RMS Error Height (m)
  Sensor:    2.034 m (RMS)
RMS Error Velocity (m/s):
  Sensor:    0.949 m/s (RMS)
```

These numbers don't mean much right now, but we will use them as a basis for comparing different approaches.

### Approach #2: A Basic Kalman Filter

Now, this may be good enough, depending on what sort of accuracy we require. But we can do better. For one thing, we can recognize and exploit the fact that the two components of our state vector, height and upward velocity, are related to each other. If we know what our height and upward velocity were at time step ```k-1``` (the previous time step, ```dt``` in the past), then we can *predict* what the height should be at time step ```k``` (right now).

#### Predicting the future

A good prediction is that height at time step ```k``` will be equal to height at ```k-1``` plus upward velocity at ```k-1``` multiplied by the interval between ```k-1``` and ```k```, which we have called ```dt```. We can capture this sort of relationship between state variables with a *state transition matrix* ```A```. ```A``` captures our understanding of the underlying system dynamics, which is typically called the *state transition model* or the *process model*. We are also going to combine the two values that we care about, height and upward velocity, into a single vector ```x```, which we will call the *state vector*.

```python
A = np.array([  # State transition matrix.
	[1, dt],
	[0, 1]
])

h = 0    # initial height: 0 m
v = 0     # initial velocity: 0 m/s

x = np.array([  # Initial state vector.
	[h],
	[v]
])
x = A @ x  # Propagate state x forward by one time step according to our model A.
```

Let's expand this equation into its scalar form and take a closer look.

```python
# Equivalent to x = A @ x
h = h + dt * v
v = v
```

Sure enough, this is exactly the relationship for height that was described in the previous paragraph. Note that our equation for v assumes that v does not change over time, which, generally, is not a reliable assumption. But remember, this is just a prediction, and for now it's the best prediction that we have, since our height at time step ```k-1``` tells us nothing about our velocity at time step ```k```. (Later on we will look at a way to improve our velocity prediction when we discuss control inputs.)

So we have our prediction for height and velocity, but we're not done. We also want to quantify how confident we are in our prediction. This confidence, represented by ```P``` (the *prediction covariance matrix*) depends on how sure we were of our previous best guess, as well as how much uncertainty there is in our model of the underlying system dynamics ```Q``` (the *process covariance matrix*). Larger values for Q indicate greater uncertainty in our understanding of the system dynamics. For example, maybe there is a cloud of space debris around us, little pieces of which are colliding with our vehicle and nudging our height and velocity.

```python
P = np.array([  # Initial state covariance.
	[1, 0],
	[0, 1]
])
Q = np.array([  # Process covariance matrix.
	[0.1, 0],
	[0, 0.1]
])
P = A @ P @ A.T + Q  # Propagate state covariance P forward by one time step according to our model A.
```

In summary, our prediction is driven by our knowledge of the underlying system dynamics: specifically it makes use of the fact that upward velocity is the time rate of change of height. What has been described here is the **first step** of a Kalman Filter, called the *prediction step*.

```python
# Predict subsequent state and covariance.
def prediction_step(x, P, A, Q):
	x = A @ x
	P = A @ P @ A.T + Q
	return x, P
```

But there's a **second step**: the *correction step*.

#### Correcting our prediction

In the prediction step we predicted the state vector at ```k``` using our previous measurements (sensor readings) and a process model. In the correction step we take new measurements and combine them with our prediction to yield our final estimate of ```x``` at ```k```. How do we combine them? Well, the short answer is that we form a weighted sum of prediction and measurements with higher weight being given for lower variance (higher certainty). The long answer is...

We start by calculating the measurement residual ```y```, which is the difference between our actual measurements ```z``` and the predicted measurements.

```python
z = np.array([
	[height_measurements[k]],
	[velocity_measurements[k]]
])
H = np.array([  # Observation matrix.
	[1, 0],
	[0, 1]
])
y = z - H @ x  # Measurement residual.
```

Note the appearance of a new matrix: ```H```. This is called the *observation matrix*, and it is responsible for mapping between state variables and the sensor readings that measure them. Since we are measuring our state variables height and velocity directly, using height and velocity sensors, ```H``` is just the identity matrix. Working with imperfect sensors is a very real and very complex topic in engineering, but as it isn't much help in understanding the core intuition behind Kalman filtering, I am going to gloss over it here. Wherever you see ```H```, you can just think of it as multiplying by 1.

Having computed the measurement residual ```y```, we then compute the residual covariance ```S``` as the sum of the prediction covariance and the measurement covariance ```R```.

```python
R = np.array([  # Measurement covariance.
	[height_variance, 0],
	[0, velocity_variance]
])
S = H @ P @ H.T + R  # Residual covariance.
```

Next we compute the Kalman gain matrix. As you may have guessed from the name, this matrix is very important, and gets to the heart of what a Kalman filter does: optimally combine all available information (measurements and system dynamics) to form the best possible estimate of the current state. The Kalman gain is the term that encapsulates this optimality. It is computed as follows.

```python
K = P @ H.T @ np.linalg.inv(S)  # Kalman gain.
```

The Kalman gain is then used to combine the predicted state with the new measurements to form the *state estimate*...

```python
x = x + K @ y  # State estimate.
```

...as well as the *state covariance estimate*.

```python
P = (np.eye(2) - K @ H) @ P  # State covariance estimate.
```

The part where we calculate our state estimate is very important, so let's look at it a little more closely. I'm going to take a few of these lines of code and rewrite them with two simplifications:

1) Ignore ```H``` as discussed above.

2) Treat matrices as scalars (replace @ with *, and np.linalg.inv() with /).

Note that this simplified version is not rigorously true, nor will it run properly with matrices, but it should look a little more intuitive.

```python
# Simplified version of state estimation step.
S = P + R
K = P / S
x = x + K * y
```

We can then rewrite this as...

```python
# Simplified, condensed version of state estimation step.
x = x + (P / (P + R)) * y
```

This says that to get our state estimate ```x``` (on the left) we take our prediction ```x``` (on the right) and multiply our measurements ```y``` by the predicted state covariance ```P``` divided by the sum of predicted state covariance and measurement covariance ```R```. If our predicted state covariance is lower than our measurement covariance, then we are more confident in our prediction than we are in our measurements, and so we weight the prediction more heavily. On the other hand, if measurement covariance is lower, then we are more confident in our measurements, and so we weight *them* more heavily.

Here is what we get for our trouble:

<img src="{{ site.baseurl }}/assets/kf/simulation-0.png" width="100%" />
*Height and velocity estimates using our basic Kalman Filter.*

```
RMS Error Height (m):
  Sensor:    2.428
  Estimator: 0.645
RMS Error Velocity (m/s):
  Sensor:    0.887
  Estimator: 0.463
```

Wow! That's a lot better than just trusting our noisy sensors. And there's still more that we can do to improve.

### Attempt #3: A Complete Kalman Filter

Reminder: Our goal is to use all information available to us to make the best possible estimate of height and velocity in the presence of noise. In the last two sections we have shown how we can make use of the information we have about our system dynamics (prediction) as well as the information we get from our measurements (correction). That's great, but there's one more source of information that we haven't yet tapped.

#### Incorporating control

Our vehicle is powered by a rocket motor that is under our control. This motor is an actuator: a device that we can command to influence our state. We are telling it what to do, which means that we have knowledge of the acceleration that it is imparting to the vehicle. We can incorporate this information into our estimate.

We do this by adjusting our prediction step to account for the fact that we are actively affecting the system dynamics. Two new terms are required. The first is the *control input*, usually denoted ```u```, which describes what each actuator is doing. Since we have only one actuator (the rocket motor), this is just a scalar rather than a vector. The second is the *control input matrix* ```B```, which describes how the control input affects the state vector. Our rocket motor imparts an upward acceleration, which in our one-dimensional world is the second time derivative of height and the first time derivative of upward velocity.

```python
u = commanded_acceleration[k]
B = np.array([
	[0.5 * dt**2],
	[dt]
])
```

Note that we're setting control input ```u``` to the *commanded* acceleration, which, if you'll remember from above, is equal to the *true* acceleration. This should raise some eyebrows and, indeed, it's a bit of a lie. What we're doing here is assuming that our rocket motor does *exactly* what we tell it to, and that we know the system dynamics *perfectly*, both of which are very naive assumptions. In reality we are subject to things like bias and noise in our actuator, as well as unexpected dynamics in our system, which can be accounted for in our ```B``` and ```Q``` matrices. Calibration of non-ideal acutators is something that I may cover in a future post. Unexpected system dynamics will be covered in the next section. For now, though, it's sufficient to consider the ideal case.

OK, all that's left is to incorporate our control into the prediction step.

```python
def prediction_step(x, P, u, A, B, Q):
    x = A @ x + B * u
    P = A @ P @ A.T + Q
    return x, P
```

Now our prediction doesn't just account for the passive dynamics of the system ```A @ x```, it also considers our active control of the dynamics via ```B * u```. This leads to the following:


<img src="{{ site.baseurl }}/assets/kf/simulation-1.png" width="100%" />
*Height and velocity estimates using our full Kalman Filter.*

```
RMS Error Height (m):
  Sensor:    2.428
  Estimator: 0.638
RMS Error Velocity (m/s):
  Sensor:    0.887
  Estimator: 0.363
```

Very nice.

### Expecting the Unexpected

As noted in the previous section, we have so far made a quite naive assumption: that our craft is not subject to any unknown, external forces. No solar radiation pressure, no magnetic forces, no micrometeorite impacts. Given that we know our surroundings so well, we should set our process covariance very low, right?

```python
Q = np.array([[0.0001, 0     ],
              [0,      0.0001]])
```

<img src="{{ site.baseurl }}/assets/kf/simulation-2.png" width="100%" />
*Estimates with very small process noise matrix.*

```
RMS Error Height (m):
  Sensor:    2.428
  Estimator: 0.554
RMS Error Velocity (m/s):
  Sensor:    0.887
  Estimator: 0.152
```

That looks suspiciously good. In fact, since we know exactly what forces our craft is subjected to (only the thrust from our rocket motor), we can predict the future perfectly and we only need sensor measurements to correct for any error in our initial estimates. Good for us.

But that's not a very realistic scenario: in the real world, unexpected things happen. Let's look at what happens if our rocket motor fails to produce any thrust for a couple of seconds.

```python
commanded_acceleration = 1 + 0.01 * np.arange(n)
true_acceleration = commanded_acceleration.copy()
true_acceleration[20:40] = 0  # Simulate 2 second failure of our rocket engine
true_velocity = np.cumsum(true_acceleration * dt)  # Integrate acceleration to get velocity
true_height = np.cumsum(true_velocity * dt)  # Integrate velocity to get height
```

<img src="{{ site.baseurl }}/assets/kf/simulation-3.png" width="100%" />
*What happens when ```Q``` is small but process noise/disturbance is large.*

```
RMS Error Height (m):
  Sensor:    2.355
  Estimator: 0.823
RMS Error Velocity (m/s):
  Sensor:    0.926
  Estimator: 0.897
```

Ouch. It takes a *very* long time for our estimator to adjust. This is because, by setting our ```Q``` matrix very close to 0, we've told our estimator to trust its process model with supreme confidence. When presented with sensor measurements that conflict with this model, it dutifully pays them very little mind, only starting to become convinced as the sensor measurements continue to disagree, over and over and over. The solution? Increase the values in our ```Q``` matrix so that the estimator doesn't trust its process model quite so zealously.

```python
Q = np.array([[0.1, 0  ],
              [0,   0.1]])
```

<img src="{{ site.baseurl }}/assets/kf/simulation-4.png" width="100%" />
*Accounting for process disturbance with a bigger ```Q```.*

```
RMS Error Height (m):
  Sensor:    2.355
  Estimator: 0.639
RMS Error Velocity (m/s):
  Sensor:    0.926
  Estimator: 0.311
```

There we go. Now keep in mind, there is a tradeoff here. Setting ```Q``` too close to zero hurts our robustness to unexpected dynamics, while setting it too *high* makes us disregard the process model entirely. Set ```Q``` high enough and our estimator will just return its sensor readings, in which case, why bother with Kalman at all.

### Knowing What You Don't Know

Let's now take a look at two parameters that we have so far glossed over: our initial state estimate ```x``` and an initial state covariance estimate ```P```. These parameters are a bit different than the others, because, of course, our estimator works to improve these estimates as time goes on. However, the choice of the *initial* values is up to us, and can have a big impact on the early performance of our estimator. Let's see what happens when we start with a *really bad* initial estimate.

```python
x_init = np.array([[100],
                   [-50]])
```

<img src="{{ site.baseurl }}/assets/kf/simulation-5.png" width="100%" />
*A bad initial estimate.*

```
RMS Error Height (m):
  Sensor:    2.147
  Estimator: 14.575
RMS Error Velocity (m/s):
  Sensor:    0.968
  Estimator: 3.444
```

It's actually pretty cool watching our estimator correct for this. But it does take some time and, during that time, our estimates are many standard deviations away from truth. This is because by setting the initial state covariance estimate ```P``` relatively close to zero, we've told our estimator to trust the initial state estimate ```x```. Let's try increasing ```P```.

```python
P_init = np.array([[100, 0 ],
                   [0,  100]])
```

<img src="{{ site.baseurl }}/assets/kf/simulation-6.png" width="100%" />
*A bad initial estimate with a correspondingly large initial ```P```.*

```
RMS Error Height (m):
  Sensor:    2.132
  Estimator: 1.109
RMS Error Velocity (m/s):
  Sensor:    1.110
  Estimator: 0.561
```

(Note that the initial estimate is being discarded in all RMS Error calculations because RMS is very sensitive to single outliers. Besides, the initial estimate is ours, not the estimator's.)

That's better. As always, it's important to consider the tradeoffs. However, in this case, the downside is minimal. If we make the initial value of ```P``` *way* too high, then our estimator variance will take a while to converge, but it will still converge. The real trouble starts when we drop the assumption of linearity and move into the dark (and much more realistic) world of nonlinear systems. More about that in a future post.

### Conclusion

We looked at three ways to estimate the height and velocity of a lunar spacecraft. First, we slapped a couple of sensors onto our craft and trusted them to tell us the height and velocity directly. This worked, but due to unavoidable sensor noise, the result was mediocre. Next, we used the magic of the Kalman Filter to combine our sensor measurements with an educated prediction of where we *should be* based on where we *were*, which improved our estimates significantly. Finally, we incorporated information about how our craft was being controlled, which improved our estimates even more. This final approach was a bona fide Kalman Filter, which combined all of the information available to us in order to form the best possible estimates. Having introduced the Kalman Filter, we looked at how this construct can be tuned in order to produce the best results.

### Recommended Further Reading

Now you know the basics, but there's much more to learn. For one thing, the Kalman Filter discussed above only works if the system dynamics (and measurements models) are linear and, as you may well know, linear systems are pretty much nonexistent in real-life engineering. To handle nonlinear systems we must turn to the Kalman Filter's nonlinear cousins: the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF). There are also more exotic state estimators like the Particle Filter that (as of this article's posting) are not yet widely used, but may become prevalent in the future. State estimation is an active field of research.

If you prefer math notation to Python, then this write-up may be more to your taste. It's a bit of a classic.

[blog link](https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)

For a more thorough treatment that puts the Kalman Filter in the context of modern control theory, Professor Steve Brunton has an excellent sequence of (39) lectures on YouTube. While you could just watch the one called "The Kalman Filter", it probably won't make much sense unless you watch the preceding ones.

[YouTube link](https://www.youtube.com/playlist?list=PLMrJAkhIeNNR20Mz-VpzgfQs5zrYi085m)

For the ambitious among you, this GitHub repository offers a practical and comprehensive view not just of the (linear) Kalman Filter, but of derived and/or related state estimators. If you want to become an expert, go here.

[GitHub link](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/tree/master)
